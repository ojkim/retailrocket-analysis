# 대용량 데이터 분석 프로젝트

이 과제는 RetailRocket의 이커머스 행동 데이터를 기반으로 고객의 행동 패턴, 구매 전환율(Funnel), 시간대별 트래픽 및 매출을 분석하는 **Kotlin 기반의 고성능 데이터 분석 애플리케이션**입니다.

수백만 건의 대용량 데이터를 처리하기 위해 **함수형 프로그래밍(Functional Programming)** 기법을 적용하면서도, **스트림(Stream)** 처리와 **내부 가변 최적화(Internal Mutability Optimization)**를 통해 메모리 효율성과 속도를 극대화했습니다.

---

## 📂 1. 데이터 파일 위치 및 준비 방법

이 과제는 **RetailRocket 데이터셋**이 필요합니다.

1.  **데이터 다운로드:** [Kaggle RetailRocket Dataset](https://www.kaggle.com/retailrocket/ecommerce-dataset)에서 데이터를 다운로드합니다.
2.  **폴더 구조:** 프로젝트 루트 경로에 `data` 폴더를 생성하고 압축을 푼 파일들을 위치시킵니다.

**필요한 파일 목록:**
* `events.csv`: 행동 데이터 (View, AddToCart, Transaction)
* `category_tree.csv`: 카테고리 구조 데이터
* `item_properties_part1.csv`: 아이템 속성 데이터 1
* `item_properties_part2.csv`: 아이템 속성 데이터 2

**디렉토리 구조 예시:**
```text
ProjectRoot/
├── src/
├── build.gradle.kts
├── README.md           
└── data/               <-- 폴더 생성 후 CSV 파일 위치
    ├── events.csv
    ├── category_tree.csv
    ├── item_properties_part1.csv
    └── item_properties_part2.csv

```
## 🚀 2. 프로젝트 실행 방법
**A. IntelliJ IDEA에서 실행하기**

1. `src/Main.kt` 파일을 엽니다.

2. `main` 함수 옆의 Run(▶) 버튼을 누르기 전, 설정을 추가해야 합니다.

3. 상단 메뉴의 Run > Edit Configurations... 로 이동합니다.

4. Program arguments 란에 데이터 폴더의 경로를 입력합니다.
   * (예시): `./data`

OK를 누르고 실행(Run)합니다.

**B. Linux / Mac (Terminal)에서 Gradle로 실행하기**

터미널을 열고 프로젝트 루트 경로에서 아래 명령어를 입력합니다.
```text
Bash

1. chmod +x gradlew

2. ./gradlew clean build

3. java -jar build/libs/retailrocket-analysis-1.0-SNAPSHOT.jar ./data
   혹은
   ./gradlew run --args="./data"
```

**C. Windows (CMD / PowerShell)에서 Gradle로 실행하기**

명령 프롬프트 또는 파워셀을 열고 아래 명령어를 입력합니다.

```text
PowerShell

1. .\gradlew.bat clean build

2. java -jar build\libs\retailrocket-analysis-1.0-SNAPSHOT.jar .\data
   혹은
   .\gradlew.bat run --args="./data"
```

## 📊 3. 주요 실행 결과
프로그램 실행 시 콘솔에 다음과 같은 리포트가 출력됩니다.

```text
=== ShopWise 데이터 분석 시스템 구동 중... ===
🌳 카테고리 구조 로딩 중...
🏷️ 아이템-카테고리 매핑 정보 생성 중... (데이터 연결)
   - 파일 처리 중: item_properties_part1.csv ... 완료 (426305 개 매핑)
   - 파일 처리 중: item_properties_part2.csv ... 완료 (361909 개 매핑)
🚀 CEO 보고를 위한 심층 데이터 분석 시작: events.csv

============================================================
📘 ShopWise 사용자 행동 분석 보고서
============================================================

1️⃣ 사용자 행동 개요 (User Actions)
   사이트 내에서 발생한 총 2,756,101건의 행동을 분석했습니다.
   • 상품 조회 (View):        2,664,312회 (96.7%)
   • 장바구니 담기 (Cart):    69,332회 ( 2.5%)
   • 구매 완료 (Order):       22,457회 ( 0.8%)
   👉 결론: 사용자의 행동 중 96% 이상은 단순 '조회'입니다.

------------------------------------------------------------
2️⃣ & 3️⃣ 구매 여정 및 이탈률 분석 (Funnel View)
   고객이 '조회 -> 장바구니 -> 구매'로 넘어갈 때 얼마나 사라지는지 보여줍니다.

   [단계 1] 상품 조회 (2,664,312명)
      │
      │  🔻 이탈: 97.4% (2,594,980명은 그냥 나감)
      │  ✅ 전환: 2.60% 만 장바구니로 이동
      ⬇️
   [단계 2] 장바구니 (69,332명)
      │
      │  🔻 이탈: 67.6% (46,875명은 결제 안 함)
      │  ✅ 전환: 32.39% 만 구매 완료
      ⬇️
   [단계 3] 구매 완료 (22,457명)

   👉 각 단계별 이탈률: 상품조회에서 장바구니 단꼐까지 고객 97.4%가 이탈하며

                     장바구니에 담은 고객 중 67.6%가 구매를 포기합니다.

------------------------------------------------------------
4️⃣ & 5️⃣ 시간에 따른 활동 패턴
   🕒 가장 활발한 쇼핑 시간 (Traffic Peak)
      - 시간: 5시
      - 규모: 187,919건의 활동 발생
      - 의미: 이때가 사이트 접속자가 가장 많습니다.

   💰 가장 많이 팔리는 시간 (Sales Peak)
      - 시간: 2시
      - 규모: 2,000건의 결제 발생
      - 의미: 실제 매출은 이때 가장 많이 일어납니다.

   👉 시간대별 구매 패턴 분석 결과:
      흥미롭게도 구경하는 시간(5시)과 구매 시간(2시)이 다릅니다.
      5시에는 상품 노출을 늘리고, 2시에는 결제 혜택을 푸시하는 전략이 필요합니다.

============================================================

(총 분석 소요 시간: 4.636초)
```
## 🤖 4. AI 도구 활용 보고서 (Gemini)

과제의 **제약 조건**, **요구 사항**, 그리고 **'나의 고민'** 을 면밀히 분석한 후, 다음과 같은 단계적인 협업 과정을 거쳤습니다.

### 🛠️ 협업 프로세스 (Workflow)

1.  **선행 분석 및 전략 수립:** 코드를 작성하기 전, 프로젝트의 핵심 난제인 **'대용량 데이터의 효율적 처리'** 아키텍처를 AI와 논의하여 전체적인 틀을 먼저 잡았습니다.
2.  **패러다임 제약 설정:** 단순한 객체지향(OOP)이 아닌, **추상화와 구조화가 갖춰진 함수형 프로그래밍(FP)** 스타일로 구현할 것을 명확히 지시했습니다.
3.  **검증 기반의 수용:** AI가 생성한 코드를 바로 적용하지 않고, **"먼저 분석해서 설명하라"**고 요청하여 논리적 타당성을 검토했습니다.
4.  **Refining (정제):** 검토 과정에서 **누락된 요구사항**, **AI가 놓친 제약조건**, **불필요하게 추가된 기능**을 식별하여 제거하고, 최적화된 코드를 기반으로 재분석을 요청했습니다.

### 💬 주요 질문 로그 (Prompts)

프로젝트 진행 과정에서 AI에게 던진 핵심 질문들은 다음과 같습니다.

1.  **아키텍처 설계:**
    > *"파일의 크기가 엄청 커. 이 데이터들을 어떻게 해야 빠르고 효율적으로 읽어와서 적용할 수 있어?"*
    * (결과: `useLines`를 활용한 스트리밍 처리 방식 채택)

2.  **전처리 전략:**
    > *"대용량 데이터의 전처리 방법과 여기에 적용할 전처리 기능들을 알려줘."*
    * (결과: 필요한 필드만 필터링(Filtering)하여 메모리에 적재하는 방식 적용)

3.  **패러다임 검증:**
    > *"중간에 객체지향인지 함수형인지 잘 모르겠어. 내가 봤을 땐 몇몇 함수가 함수형을 사용해서 함수형 코드 같은데 분석해줘."*
    * (결과: `fold`, `map` 등 고차 함수 활용 확인 및 내부 가변/외부 불변 패턴 확립)

4.  **최종 적합성 평가:**
    > *"이 코드를 다시 분석하고 내가 전에 말해준 요구사항과 제약조건에 맞는 코드인지 확인해봐."*
    * (결과: 과제 요건 충족 여부 및 성능 최적화 최종 점검)
```text
실제로 한 중요하다고 생각한 질문들

1. 파일의 크기가 엄청 커 이 데이터들을 어떻게 해야 빠르고 효율적으로 읽어와서 적용할 수 있어?

2. 대용량 데이터의 전처리 방법과 여기에 적용할 전처리 기능들을 알려줘

3. 중간에 객체지향인지 함수형인지 잘 모르겠어 내가봤을땐 몇몇 함수가 함수형을 사용해서 함수형 코드 같아 분석해줘

4. 이 코드를 다시 분석하고 내가 전에 말해준 요구사항과 제약조건에 맞는 코드인지 확인해봐
```


